{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c35e248",
   "metadata": {},
   "source": [
    "**Skip to Content**\n",
    "  __\n",
    " SAP Community Log-in Update\n",
    " In a few months, SAP Community will switch to SAP Universal ID as the only option to login. Don’t wait, create your SAP Universal ID\n",
    " now\\! If you have multiple S- or P- accounts, use the Consolidation Tool to merge your content.\n",
    " Get started with SAP Universal ID\n",
    "  __\n",
    "   - Home\n",
    "   - Community\n",
    "   - Ask a Question\n",
    "   - Write a Blog Post\n",
    "   - Login / Sign-up\n",
    "   -\n",
    " ###### Technical Articles\n",
    " Yannick Schaper\n",
    " __\n",
    " February 25, 2021 7 minute read\n",
    " # Hands-On Tutorial: Leverage SAP HANA Machine Learning in the Cloud through the Predictive Analysis Library\n",
    " __7 __32 __3,258\n",
    " The hard truth is that many machine learning projects fail to get set into production. It takes time and real effort to move from a\n",
    " machine learning model to a real business application. This is due to many different reasons, for example:\n",
    "   1. Limited data access\n",
    "   2. Poor data quality\n",
    "   3. Small computing power\n",
    "   4. No version control\n",
    " Of course, we can’t save the world with just one Hands-On tutorial, but we can at least try to make the life of a data scientist a\n",
    " little easier. **In this blog post we will tackle these challenges by bringing the opensource world and SAP world together.** In a\n",
    " nutshell, there will be no movement of training data from SAP HANA Cloud to our Python environment. In the following, we will refer\n",
    " to our SAP HANA Cloud simply as HANA. Of course, you can access the data and sample code under the following GitHub repository. But\n",
    " let us start from the beginning\\!\n",
    "   1. **How can we directly access the data in our HANA?**\n",
    " As data scientists we feel very comfortable in our RStudio or Python Jupyter Notebooks. We want to stay in our used environment and\n",
    " don’t want to move around in too many different tools. Or even worse, we must copy the code or data from A to B, working on our\n",
    " local laptop with different csv files or other formats. Hence, the great news is that we can directly connect through the R & Python\n",
    " clients with our HANA. Let us look at an example.\n",
    " In our Python script we first install and import the following library:\n",
    " The hana\\_ml library enables us to directly connect to our HANA. To leverage it’s full potential we have to make sure that our user\n",
    " has the following policies assigned:\n",
    "   1. AFL\\_\\_SYS\\_AFL\\_AFLPAL\\_EXECUTE\\_WITH\\_GRANT\\_OPTION\n",
    "   2. AFL\\_\\_SYS\\_AFL\\_APL\\_AREA\\_EXECUTE\n",
    "   3. AFLPM\\_CREATOR\\_ERASER\\_EXECUTE\n",
    " Set your HANA host, port, user, password and encrypt to true:\n",
    " Execute the following command to connect to your HANA:\n",
    " We can hide our login credentials through the Secure User store from the HANA client and don’t have them visible in clear text. In\n",
    " our command prompt we execute the following script:\n",
    " C:\\Program Files\\SAP\\hdbclient>hdbuserstore -i SET MYHANACLOUD “YOURENDPOINT:PORT” YOURUSERNAME\n",
    " Then back in our Python script we can use the HANA key to connect:\n",
    " Now, let us upload a local dataset and push it directly into HANA. Make sure you change the path to your local directory.\n",
    " Before we bring our local dataset into HANA, we must execute some transformations. We change the columns to upper string and add a\n",
    " unique Product ID to the data. This ID will later be used as a key in our machine learning algorithms, which are directly running in\n",
    " our HANA.\n",
    " Next, let us create a HANA dataframe and point it to the table with the uploaded data.\n",
    " Great job\\! We tackled our first challenge in our machine learning use case. Since, we are now able to directly connect to our HANA\n",
    " from our used environment, let us move to the next task.\n",
    "   2. **How can we explore our data and react to data quality issues early?**\n",
    " Data understanding and preparation take up a lot off time during a machine learning use case. Increasing the data quality can drive\n",
    " every data scientist crazy, due to so many reasons. Hence, let us look at some options we have now through the hana\\_ml library.\n",
    " Right away our HANA dataframe provides us with different functionalities. Therefore, let us control the data size and if all the\n",
    " variable types are set correctly:\n",
    " Our data upload was complete but since the variable Quality only contains 0s and 1s, it was falsely set to type integer. The\n",
    " variable is binary and labels all products of bad quality with a 1 and 0 otherwise. Since this is by definition a categorical\n",
    " variable, we transform it to type NVARCHAR with the following command.\n",
    " To better understand the data, we can derive a description of different statistical attributes or even get a whole data report\n",
    " through the following commands:\n",
    " If we now find any errors, we have many different functions on hand, staying in our Python environment. For example, we can execute\n",
    " sql commands directly in our Python script or use many more functions available with our HANA dataframe.\n",
    "   3. **How can we leverage the computing power of our HANA in our machine learning use case?**\n",
    " At some point we must bring the data and the algorithms together to train our machine learning models. Since our HANA is not only an\n",
    " in-memory database, we don’t have to collect the data into our local environment. Instead we can leverage the native machine\n",
    " learning libraries directly in our HANA. Therefore, let us split the data into a training and testing set through the Predictive\n",
    " Analysis Library \\(PAL\\).\n",
    " Further, let us train and optimize a random forest algorithm to classify the quality of a product. First, we set the numbers of\n",
    " trees very high, to see where the Out of Bag error converges. After optimizing the numbers of trees, we will take a closer look at\n",
    " the variables considered at each split.\n",
    " Now, we can apply our trained model on the testing set.\n",
    " For each observation we can see the predicted class and of course the confidence of our random forest model. In addition, we can\n",
    " compute the confusion matrix or a have a look at the variable importance.\n",
    " The idea is to keep the heavy lifting in our HANA and only collect small sets to for example evaluate our random forest model.\n",
    " Therefore, we can collect the OOB error and use opensource libraries to visualize the results:\n",
    " Our trained model converges after around 700 trees. Hence, let us set the parameter to 801 trees to leave a little buffer. Further,\n",
    " let us optimize the number of variables considered at each split by iterating through each possible value.\n",
    " Let’s plot the result:\n",
    " After 2 variables considered at each split the error increases and we must be careful not to overfit the data. Therefore, let us\n",
    " train our optimal random forest model.\n",
    " Congratulation, you created your first machine learning model directly in HANA. Now, let’s move to our last pain point.\n",
    "   4. **How can we save and create different versions of our results?**\n",
    " After the evaluation of our machine learning model a last very crucial step is still missing. We must deploy our model into the IT\n",
    " landscape and make our new insights accessible to the business, such that they can incorporate this information into their decision\n",
    " making. Our huge advantage is now that our machine learning model already resides in HANA. We can create a Model Storage where we\n",
    " can save our trained model and load it again for consumption. Hence, please execute the following script:\n",
    " Let’s view the saved model in our model storage.\n",
    " Of course, we can now save additional tables created based on our model and consume them in different frontend tools like the SAP\n",
    " Analytics Cloud. For example, have a look at the following blogs:\n",
    "   - Story design, formatting, and aesthetics\n",
    "   - SAP Analytics Cloud Learning\n",
    "   - Hands-On Tutorial SAP Analytics Cloud, R Visualization\n",
    " In addition, if you like to try the Automated Predictive Library \\(APL\\) or are a big R fan, try out the following Hands-On\n",
    " tutorials:\n",
    "   - Hands-On Tutorial: Automated Predictive \\(APL\\) in SAP HANA Cloud\n",
    "   - Hands-On Tutorial: Leverage SAP HANA embedded Machine Learning through an R Shiny App\n",
    "   - Hands-On Tutorial: Becoming the Chief Data Cook with RStudio and SAP HANA\n",
    " I hope this blog post helped you to get started with your own SAP Machine Learning use cases. I encourage you to try it yourself and\n",
    " bring more machine learning into production.\n",
    " I want to thank Andreas Forster, Christoph Morgen and Sarah Detzler for their support while writing this Hands-On tutorial.\n",
    " Cheers\\!\n",
    " Yannick Schaper\n",
    " Follow __Like __RSS Feed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
